# ğŸš€ Azure Data Factory ETL Demo with Azure Developer CLI

This project demonstrates a **real-world ETL (Extract-Transform-Load)** pipeline using **Azure Data Factory (ADF)**, **Azure Storage (Data Lake Gen2)**, and **Azure SQL Database** â€” all deployed automatically using **Azure Developer CLI (azd)** and **modular Bicep templates**.

---

## ğŸ§© Scenario Overview

A retail company collects daily sales data from multiple branches as CSV files stored in an on-premises system (simulated here using Azure Blob Storage). The goal is to:

1. **Extract** raw data from the â€œon-premisesâ€ folder (Blob container `raw`)
2. **Transform** the data in Azure Data Factory by:
   - Joining with store reference data
   - Cleaning missing values
   - Calculating profit
3. **Load** the processed data into an **Azure SQL Database**
4. (Optional) **Visualize** the results in Power BI

---

## ğŸ—ï¸ Architecture

```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  On-prem / Raw Data (CSV)   â”‚
   â”‚  â†’ Azure Storage (raw/)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Azure Data Factory (ADF)    â”‚
        â”‚  â€¢ Copy data to staging     â”‚
        â”‚  â€¢ Transform (join, profit) â”‚
        â”‚  â€¢ Load to SQL DB           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Azure SQL Database        â”‚
          â”‚  â€¢ Cleaned Sales Data     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
azure-etl-adf-demo/
â”‚
â”œâ”€â”€ azd.yaml                # Azure Developer CLI config
â”œâ”€â”€ README.md               # This file
â””â”€â”€ infra/
    â”œâ”€â”€ main.bicep          # Subscription-level deployment entry point
    â”œâ”€â”€ modules/
    â”‚   â”œâ”€â”€ storage.bicep   # Storage account (Data Lake Gen2)
    â”‚   â”œâ”€â”€ sql.bicep       # SQL Server + Database
    â”‚   â””â”€â”€ datafactory.bicep  # Azure Data Factory + Linked Service
```

---

## ğŸ§± Deployed Resources

| Resource | Purpose |
|-----------|----------|
| **Resource Group** | Container for all demo resources |
| **Storage Account** | Simulates on-prem/raw data source (`raw` container) |
| **Azure SQL Database** | Destination for cleaned, transformed data |
| **Azure Data Factory** | ETL orchestration and transformation layer |

---

## âš™ï¸ Prerequisites

Make sure you have:

- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed  
- [Azure Developer CLI (azd)](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd) installed  
- Permissions to create resource groups and resources in your Azure subscription  

---

## ğŸš€ Deployment Steps

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/koenraadhaedens/azd-ETL-with-Azure-Data-Factory
cd azure-etl-adf-demo
```

### 2ï¸âƒ£ Initialize the Project

```bash
azd init --template .
```

### 3ï¸âƒ£ Log In to Azure

```bash
azd auth login
```

### 4ï¸âƒ£ Provision the Infrastructure

```bash
azd up
```

During deployment:
- Youâ€™ll be asked to enter:
  - `environmentName` (e.g., `dev`, `test`, `demo`)
  - `location` (e.g., `westeurope`)
  - SQL administrator password (stored securely)

This will:
- Create a new resource group (`rg-<environmentName>`)
- Deploy Storage, SQL, and Data Factory
- Output resource names and connection strings

Example output:
```
âœ” Resource group: rg-demo
âœ” Storage account: demoestore
âœ” SQL server: demoesql
âœ” SQL database: salesdb
âœ” Data Factory: demoeadf
```

---

## ğŸ§ª Post-Deployment Steps

1. **Upload Sample Data**

   Upload your CSV files (e.g., `sales.csv` and `stores.csv`) to the `raw` container of your storage account:

   ```bash
   az storage blob upload-batch      --account-name <storageAccountName>      --source ./data      --destination raw
   ```

2. **Open Azure Data Factory Studio**

   In the Azure Portal, navigate to your Data Factory instance â†’ **"Launch Studio"**  
   Create a new pipeline that:
   - Copies data from the Blob storage `raw` container to a staging area
   - Uses a **Mapping Data Flow** to join and clean data
   - Loads output to your SQL database

3. **Verify Results**

   Query the SQL database:

   ```sql
   SELECT TOP 10 * FROM dbo.SalesFact;
   ```

4. *(Optional)* Connect Power BI Desktop to the SQL Database for reporting.

---

## ğŸ§¹ Cleanup

When youâ€™re done, remove all resources:

```bash
azd down
```

---

## ğŸ§  Key Learnings

- How to **orchestrate ETL pipelines** using Azure Data Factory  
- How to **modularize infrastructure as code** with Bicep and `azd`  
- How to deploy consistent environments automatically using `azd up`  

---

## ğŸª„ Next Steps

You can extend this demo by:
- Adding a **post-provision step** to upload CSVs automatically  
- Deploying a **Data Factory pipeline definition** (JSON ARM resource)  
- Integrating **Power BI** for reporting  

---

**Author:** Kenraad Haedens
**Demo Type:** ETL with Azure Data Factory  
**Deployment Tool:** Azure Developer CLI (azd)  
**Duration:** ~15 minutes for full setup
